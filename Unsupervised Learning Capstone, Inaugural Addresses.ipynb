{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuelkim/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import inaugural, stopwords\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.ensemble import RandomForestClassifier                          \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import SVC   \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning Capstone: Classifying 1st Inaugural Addresses (1909-2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "US Presidents have used their 1st Inaugural Addresses as an opportunity to highlight their objectives and hopes for the future as Commander-in-chief. For this project, I wanted to analyze the thirteen most recent first inaugural addresses of past US presidents over the last century - from President Taft to President Obama's. We will try to classify the presidents and their addresses using a number of Supervised and Unsupervised Learning tools. The texts were pulled from NLTK. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     /Users/samuelkim/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n",
      "['1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', '1801-Jefferson.txt', '1805-Jefferson.txt', '1809-Madison.txt', '1813-Madison.txt', '1817-Monroe.txt', '1821-Monroe.txt', '1825-Adams.txt', '1829-Jackson.txt', '1833-Jackson.txt', '1837-VanBuren.txt', '1841-Harrison.txt', '1845-Polk.txt', '1849-Taylor.txt', '1853-Pierce.txt', '1857-Buchanan.txt', '1861-Lincoln.txt', '1865-Lincoln.txt', '1869-Grant.txt', '1873-Grant.txt', '1877-Hayes.txt', '1881-Garfield.txt', '1885-Cleveland.txt', '1889-Harrison.txt', '1893-Cleveland.txt', '1897-McKinley.txt', '1901-McKinley.txt', '1905-Roosevelt.txt', '1909-Taft.txt', '1913-Wilson.txt', '1917-Wilson.txt', '1921-Harding.txt', '1925-Coolidge.txt', '1929-Hoover.txt', '1933-Roosevelt.txt', '1937-Roosevelt.txt', '1941-Roosevelt.txt', '1945-Roosevelt.txt', '1949-Truman.txt', '1953-Eisenhower.txt', '1957-Eisenhower.txt', '1961-Kennedy.txt', '1965-Johnson.txt', '1969-Nixon.txt', '1973-Nixon.txt', '1977-Carter.txt', '1981-Reagan.txt', '1985-Reagan.txt', '1989-Bush.txt', '1993-Clinton.txt', '1997-Clinton.txt', '2001-GWBush.txt', '2005-Bush.txt', '2009-Obama.txt']\n"
     ]
    }
   ],
   "source": [
    "nltk.download('inaugural')\n",
    "print(inaugural.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My fellow citizens : Anyone who has taken the oath I have just taken must feel a heavy weight of responsibility .',\n",
       " 'If not , he has no conception of the powers and duties of the office upon which he is about to enter , or he is lacking in a proper sense of the obligation which the oath imposes .',\n",
       " 'The office of an inaugural address is to give a summary outline of the main policies of the new administration , so far as they can be anticipated .',\n",
       " 'I have had the honor to be one of the advisers of my distinguished predecessor , and , as such , to hold up his hands in the reforms he has initiated .',\n",
       " 'I should be untrue to myself , to my promises , and to the declarations of the party platform upon which I was elected to office , if I did not make the maintenance and enforcement of those reforms a most important feature of my administration .']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We are going to pick the 18 inaugural addresses we wish to analyze and put them into a list; \n",
    "#after cleaning it, we will create sentence-level documents for each text. \n",
    "#The addresses will then be tied to the names of the President that delivered it\n",
    "#which we extracted from the txt file titles, i.e. file (5:-4) for '1969-Nixon.txt'. \n",
    "\n",
    "#nltk.rename('2001-Bush.txt', '2001-GWBush.txt')\n",
    "\n",
    "labels = []\n",
    "\n",
    "file_ids = ['1909-Taft.txt', '1913-Wilson.txt', '1917-Wilson.txt', '1921-Harding.txt', \n",
    "            '1925-Coolidge.txt', '1929-Hoover.txt', '1933-Roosevelt.txt', '1949-Truman.txt', '1953-Eisenhower.txt', '1961-Kennedy.txt', \n",
    "            '1965-Johnson.txt', '1969-Nixon.txt', '1977-Carter.txt', '1981-Reagan.txt', '1989-Bush.txt', \n",
    "            '1993-Clinton.txt', '2001-GWBush.txt', '2009-Obama.txt']\n",
    "for file in file_ids:\n",
    "    president = re.sub(\"[^a-zA-Z]\", '', file[5:-4])\n",
    "    labels.append([file, president])\n",
    "\n",
    "sent_list = []\n",
    "pres_list = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    sents = inaugural.sents(labels[i][0])\n",
    "    joined_sents = [(' '.join(sent), labels[i][1]) for sent in sents]\n",
    "    for i in range(len(joined_sents)): \n",
    "        sent_list.append(joined_sents[i][0]) \n",
    "        pres_list.append(joined_sents[i][1])  \n",
    "\n",
    "sent_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['My fellow citizens   Anyone who has taken the oath I have just taken must feel a heavy weight of responsibility  ',\n",
       " 'If not   he has no conception of the powers and duties of the office upon which he is about to enter   or he is lacking in a proper sense of the obligation which the oath imposes  ',\n",
       " 'The office of an inaugural address is to give a summary outline of the main policies of the new administration   so far as they can be anticipated  ',\n",
       " 'I have had the honor to be one of the advisers of my distinguished predecessor   and   as such   to hold up his hands in the reforms he has initiated  ',\n",
       " 'I should be untrue to myself   to my promises   and to the declarations of the party platform upon which I was elected to office   if I did not make the maintenance and enforcement of those reforms a most important feature of my administration  ']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we will also replace upper-case sentences, punctuations and numeric values with \"\" or ' '\n",
    "sent_list_clean = []\n",
    "for sent in sent_list:\n",
    "    sent = re.sub(\"[^a-zA-Z]\", ' ', sent) \n",
    "    if sent == sent.upper():              \n",
    "        sent = \"\"                         \n",
    "    sent_list_clean.append(sent)\n",
    "print(len(sent_list_clean))\n",
    "sent_list_clean[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My fellow citizen Anyone who ha taken the oath I have just taken must feel a heavy weight of responsibility',\n",
       " 'If not he ha no conception of the power and duty of the office upon which he is about to enter or he is lacking in a proper sense of the obligation which the oath imposes',\n",
       " 'The office of an inaugural address is to give a summary outline of the main policy of the new administration so far a they can be anticipated',\n",
       " 'I have had the honor to be one of the adviser of my distinguished predecessor and a such to hold up his hand in the reform he ha initiated',\n",
       " 'I should be untrue to myself to my promise and to the declaration of the party platform upon which I wa elected to office if I did not make the maintenance and enforcement of those reform a most important feature of my administration']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize and lemmatize words, as well as getting rid of ' ' formed by the previous code.\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemma_sents = []\n",
    "for sent in sent_list_clean:\n",
    "    words = word_tokenize(sent)                                 \n",
    "    word_lemma = [lemmatizer.lemmatize(word) for word in words] \n",
    "    sent_lemma = ' '.join(word_lemma)                           \n",
    "    lemma_sents.append(sent_lemma)\n",
    "lemma_sents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>president</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My fellow citizen Anyone who ha taken the oath...</td>\n",
       "      <td>Taft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If not he ha no conception of the power and du...</td>\n",
       "      <td>Taft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The office of an inaugural address is to give ...</td>\n",
       "      <td>Taft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have had the honor to be one of the adviser ...</td>\n",
       "      <td>Taft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I should be untrue to myself to my promise and...</td>\n",
       "      <td>Taft</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sent president\n",
       "0  My fellow citizen Anyone who ha taken the oath...      Taft\n",
       "1  If not he ha no conception of the power and du...      Taft\n",
       "2  The office of an inaugural address is to give ...      Taft\n",
       "3  I have had the honor to be one of the adviser ...      Taft\n",
       "4  I should be untrue to myself to my promise and...      Taft"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can now create a dataframe with our sentences and their associated presidents\n",
    "df = pd.DataFrame()\n",
    "df['sent'] = lemma_sents \n",
    "df['president'] = pres_list\n",
    "df = df[df.sent!=\"\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr Chief Justice Mr President Vice President Quayle Senator Mitchell Speaker Wright Senator Dole Congressman Michael and fellow citizen neighbor and friend',\n",
       " 'There is a man here who ha earned a lasting place in our heart and in our history',\n",
       " 'President Reagan on behalf of our Nation I thank you for the wonderful thing that you have done for America',\n",
       " 'I have just repeated word for word the oath taken by George Washington year ago and the Bible on which I placed my hand is the Bible on which he placed his',\n",
       " 'It is right that the memory of Washington be with u today not only because this is our Bicentennial Inauguration but because Washington remains the Father of our Country',\n",
       " 'And he would I think be gladdened by this day for today is the concrete expression of a stunning fact our continuity these year since our government began',\n",
       " 'We meet on democracy s front porch a good place to talk a neighbor and a friend',\n",
       " 'For this is a day when our nation is made whole when our difference for a moment are suspended',\n",
       " 'And my first act a President is a prayer',\n",
       " 'I ask you to bow your head',\n",
       " 'Heavenly Father we bow our head and thank You for Your love',\n",
       " 'Accept our thanks for the peace that yield this day and the shared faith that make it continuance likely',\n",
       " 'Make u strong to do Your work willing to heed and hear Your will and write on our heart these word Use power to help people',\n",
       " 'For we are given power not to advance our own purpose nor to make a great show in the world nor a name',\n",
       " 'There is but one just use of power and it is to serve people',\n",
       " 'Help u to remember it Lord',\n",
       " 'Amen',\n",
       " 'I come before you and assume the Presidency at a moment rich with promise',\n",
       " 'We live in a peaceful prosperous time but we can make it better',\n",
       " 'For a new breeze is blowing and a world refreshed by freedom seems reborn for in man s heart if not in fact the day of the dictator is over',\n",
       " 'The totalitarian era is passing it old idea blown away like leaf from an ancient lifeless tree',\n",
       " 'A new breeze is blowing and a nation refreshed by freedom stand ready to push on',\n",
       " 'There is new ground to be broken and new action to be taken',\n",
       " 'There are time when the future seems thick a a fog you sit and wait hoping the mist will lift and reveal the right path',\n",
       " 'But this is a time when the future seems a door you can walk right through into a room called tomorrow',\n",
       " 'Great nation of the world are moving toward democracy through the door to freedom',\n",
       " 'Men and woman of the world move toward free market through the door to prosperity',\n",
       " 'The people of the world agitate for free expression and free thought through the door to the moral and intellectual satisfaction that only liberty allows',\n",
       " 'We know what work Freedom work',\n",
       " 'We know what s right Freedom is right',\n",
       " 'We know how to secure a more just and prosperous life for man on Earth through free market free speech free election and the exercise of free will unhampered by the state',\n",
       " 'For the first time in this century for the first time in perhaps all history man doe not have to invent a system by which to live',\n",
       " 'We don t have to talk late into the night about which form of government is better',\n",
       " 'We don t have to wrest justice from the king',\n",
       " 'We only have to summon it from within ourselves',\n",
       " 'We must act on what we know',\n",
       " 'I take a my guide the hope of a saint In crucial thing unity in important thing diversity in all thing generosity',\n",
       " 'America today is a proud free nation decent and civil a place we can not help but love',\n",
       " 'We know in our heart not loudly and proudly but a a simple fact that this country ha meaning beyond what we see and that our strength is a force for good',\n",
       " 'But have we changed a a nation even in our time',\n",
       " 'Are we enthralled with material thing le appreciative of the nobility of work and sacrifice',\n",
       " 'My friend we are not the sum of our possession',\n",
       " 'They are not the measure of our life',\n",
       " 'In our heart we know what matter',\n",
       " 'We can not hope only to leave our child a bigger car a bigger bank account',\n",
       " 'We must hope to give them a sense of what it mean to be a loyal friend a loving parent a citizen who leaf his home his neighborhood and town better than he found it',\n",
       " 'What do we want the men and woman who work with u to say when we are no longer there',\n",
       " 'That we were more driven to succeed than anyone around u',\n",
       " 'Or that we stopped to ask if a sick child had gotten better and stayed a moment there to trade a word of friendship',\n",
       " 'No President no government can teach u to remember what is best in what we are',\n",
       " 'But if the man you have chosen to lead this government can help make a difference if he can celebrate the quieter deeper success that are made not of gold and silk but of better heart and finer soul if he can do these thing then he must',\n",
       " 'America is never wholly herself unless she is engaged in high moral principle',\n",
       " 'We a a people have such a purpose today',\n",
       " 'It is to make kinder the face of the Nation and gentler the face of the world',\n",
       " 'My friend we have work to do',\n",
       " 'There are the homeless lost and roaming',\n",
       " 'There are the child who have nothing no love no normalcy',\n",
       " 'There are those who can not free themselves of enslavement to whatever addiction drug welfare the demoralization that rule the slum',\n",
       " 'There is crime to be conquered the rough crime of the street',\n",
       " 'There are young woman to be helped who are about to become mother of child they can t care for and might not love',\n",
       " 'They need our care our guidance and our education though we bless them for choosing life',\n",
       " 'The old solution the old way wa to think that public money alone could end these problem',\n",
       " 'But we have learned that is not so',\n",
       " 'And in any case our fund are low',\n",
       " 'We have a deficit to bring down',\n",
       " 'We have more will than wallet but will is what we need',\n",
       " 'We will make the hard choice looking at what we have and perhaps allocating it differently making our decision based on honest need and prudent safety',\n",
       " 'And then we will do the wisest thing of all We will turn to the only resource we have that in time of need always grows the goodness and the courage of the American people',\n",
       " 'I am speaking of a new engagement in the life of others a new activism hand on and involved that get the job done',\n",
       " 'We must bring in the generation harnessing the unused talent of the elderly and the unfocused energy of the young',\n",
       " 'For not only leadership is passed from generation to generation but so is stewardship',\n",
       " 'And the generation born after the Second World War ha come of age',\n",
       " 'I have spoken of a thousand point of light of all the community organization that are spread like star throughout the Nation doing good',\n",
       " 'We will work hand in hand encouraging sometimes leading sometimes being led rewarding',\n",
       " 'We will work on this in the White House in the Cabinet agency',\n",
       " 'I will go to the people and the program that are the brighter point of light and I will ask every member of my government to become involved',\n",
       " 'The old idea are new again because they are not old they are timeless duty sacrifice commitment and a patriotism that find it expression in taking part and pitching in',\n",
       " 'We need a new engagement too between the Executive and the Congress',\n",
       " 'The challenge before u will be thrashed out with the House and the Senate',\n",
       " 'We must bring the Federal budget into balance',\n",
       " 'And we must ensure that America stand before the world united strong at peace and fiscally sound',\n",
       " 'But of course thing may be difficult',\n",
       " 'We need compromise we have had dissension',\n",
       " 'We need harmony we have had a chorus of discordant voice',\n",
       " 'For Congress too ha changed in our time',\n",
       " 'There ha grown a certain divisiveness',\n",
       " 'We have seen the hard look and heard the statement in which not each other s idea are challenged but each other s motif',\n",
       " 'And our great party have too often been far apart and untrusting of each other',\n",
       " 'It ha been this way since Vietnam',\n",
       " 'That war cleaves u still',\n",
       " 'But friend that war began in earnest a quarter of a century ago and surely the statute of limitation ha been reached',\n",
       " 'This is a fact The final lesson of Vietnam is that no great nation can long afford to be sundered by a memory',\n",
       " 'A new breeze is blowing and the old bipartisanship must be made new again',\n",
       " 'To my friend and yes I do mean friend in the loyal opposition and yes I mean loyal I put out my hand',\n",
       " 'I am putting out my hand to you Mr Speaker',\n",
       " 'I am putting out my hand to you Mr',\n",
       " 'Majority Leader',\n",
       " 'For this is the thing This is the age of the offered hand',\n",
       " 'We can t turn back clock and I don t want to',\n",
       " 'But when our father were young Mr Speaker our difference ended at the water s edge',\n",
       " 'And we don t wish to turn back time but when our mother were young Mr',\n",
       " 'Majority Leader the Congress and the Executive were capable of working together to produce a budget on which this nation could live',\n",
       " 'Let u negotiate soon and hard',\n",
       " 'But in the end let u produce',\n",
       " 'The American people await action',\n",
       " 'They didn t send u here to bicker',\n",
       " 'They ask u to rise above the merely partisan',\n",
       " 'In crucial thing unity and this my friend is crucial',\n",
       " 'To the world too we offer new engagement and a renewed vow We will stay strong to protect the peace',\n",
       " 'The offered hand is a reluctant fist but once made strong and can be used with great effect',\n",
       " 'There are today Americans who are held against their will in foreign land and Americans who are unaccounted for',\n",
       " 'Assistance can be shown here and will be long remembered',\n",
       " 'Good will begets good will',\n",
       " 'Good faith can be a spiral that endlessly move on',\n",
       " 'Great nation like great men must keep their word',\n",
       " 'When America say something America mean it whether a treaty or an agreement or a vow made on marble step',\n",
       " 'We will always try to speak clearly for candor is a compliment but subtlety too is good and ha it place',\n",
       " 'While keeping our alliance and friendship around the world strong ever strong we will continue the new closeness with the Soviet Union consistent both with our security and with progress',\n",
       " 'One might say that our new relationship in part reflects the triumph of hope and strength over experience',\n",
       " 'But hope is good and so are strength and vigilance',\n",
       " 'Here today are ten of thousand of our citizen who feel the understandable satisfaction of those who have taken part in democracy and seen their hope fulfilled',\n",
       " 'But my thought have been turning the past few day to those who would be watching at home to an older fellow who will throw a salute by himself when the flag go by and the woman who will tell her son the word of the battle hymn',\n",
       " 'I don t mean this to be sentimental',\n",
       " 'I mean that on day like this we remember that we are all part of a continuum inescapably connected by the tie that bind',\n",
       " 'Our child are watching in school throughout our great land',\n",
       " 'And to them I say thank you for watching democracy s big day',\n",
       " 'For democracy belongs to u all and freedom is like a beautiful kite that can go higher and higher with the breeze',\n",
       " 'And to all I say No matter what your circumstance or where you are you are part of this day you are part of the life of our great nation',\n",
       " 'A President is neither prince nor pope and I don t seek a window on men s soul',\n",
       " 'In fact I yearn for a greater tolerance an easy goingness about each other s attitude and way of life',\n",
       " 'There are few clear area in which we a a society must rise up united and express our intolerance',\n",
       " 'The most obvious now is drug',\n",
       " 'And when that first cocaine wa smuggled in on a ship it may a well have been a deadly bacteria so much ha it hurt the body the soul of our country',\n",
       " 'And there is much to be done and to be said but take my word for it This scourge will stop',\n",
       " 'And so there is much to do and tomorrow the work begin',\n",
       " 'I do not mistrust the future I do not fear what is ahead',\n",
       " 'For our problem are large but our heart is larger',\n",
       " 'Our challenge are great but our will is greater',\n",
       " 'And if our flaw are endless God s love is truly boundless',\n",
       " 'Some see leadership a high drama and the sound of trumpet calling and sometimes it is that',\n",
       " 'But I see history a a book with many page and each day we fill a page with act of hopefulness and meaning',\n",
       " 'The new breeze blow a page turn the story unfolds',\n",
       " 'And so today a chapter begin a small and stately story of unity diversity and generosity shared and written together',\n",
       " 'Thank you',\n",
       " 'God bless you and God bless the United States of America']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we now have a giant document for all 18 texts, but with presidents' names distinguishing them\n",
    "#I created a dict in case we need to study individual addresses more closely \n",
    "president_speech_dict={}\n",
    "for president in df.president.unique():\n",
    "    president_speech_dict[president] = df[df.president == president].sent.values.tolist()\n",
    "president_speech_dict['Bush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "135\n",
      "131\n",
      "57\n",
      "87\n",
      "76\n",
      "217\n",
      "13\n",
      "47\n",
      "57\n",
      "74\n",
      "182\n",
      "48\n",
      "167\n",
      "63\n",
      "174\n",
      "54\n",
      "34\n",
      "54\n",
      "38\n",
      "102\n",
      "166\n",
      "98\n",
      "290\n",
      "88\n",
      "17\n",
      "60\n",
      "127\n",
      "202\n",
      "99\n",
      "106\n",
      "85\n",
      "131\n",
      "127\n",
      "37\n",
      "54\n",
      "59\n",
      "114\n",
      "33\n",
      "121\n",
      "104\n",
      "47\n",
      "137\n",
      "111\n",
      "123\n",
      "85\n",
      "91\n",
      "18\n",
      "19\n",
      "120\n",
      "22\n",
      "163\n",
      "159\n",
      "200\n",
      "49\n",
      "39\n",
      "229\n",
      "294\n",
      "264\n",
      "87\n",
      "177\n",
      "84\n",
      "288\n",
      "147\n",
      "93\n",
      "187\n",
      "32\n",
      "154\n",
      "97\n",
      "129\n",
      "275\n",
      "68\n",
      "74\n",
      "460\n",
      "89\n",
      "178\n",
      "205\n",
      "157\n",
      "201\n",
      "51\n",
      "166\n",
      "97\n",
      "180\n",
      "113\n",
      "140\n",
      "190\n",
      "150\n",
      "24\n",
      "49\n",
      "157\n",
      "20\n",
      "65\n",
      "48\n",
      "359\n",
      "48\n",
      "99\n",
      "310\n",
      "82\n",
      "133\n",
      "24\n",
      "22\n",
      "30\n",
      "129\n",
      "187\n",
      "2\n",
      "7\n",
      "98\n",
      "89\n",
      "285\n",
      "9\n",
      "13\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "#each individual sentence length, after data cleaning\n",
    "for president in president_speech_dict[president]:\n",
    "    print(len(president))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization and Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed with some unsupervised feature generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Coolidge</th>\n",
       "      <td>0.099395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taft</th>\n",
       "      <td>0.080222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hoover</th>\n",
       "      <td>0.079717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Harding</th>\n",
       "      <td>0.075177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bush</th>\n",
       "      <td>0.073158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilson</th>\n",
       "      <td>0.064581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reagan</th>\n",
       "      <td>0.064077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eisenhower</th>\n",
       "      <td>0.062059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Truman</th>\n",
       "      <td>0.058527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obama</th>\n",
       "      <td>0.056509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nixon</th>\n",
       "      <td>0.053481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GWBush</th>\n",
       "      <td>0.048940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Johnson</th>\n",
       "      <td>0.047427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roosevelt</th>\n",
       "      <td>0.042886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clinton</th>\n",
       "      <td>0.040868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carter</th>\n",
       "      <td>0.026741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kennedy</th>\n",
       "      <td>0.026236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                sent\n",
       "president           \n",
       "Coolidge    0.099395\n",
       "Taft        0.080222\n",
       "Hoover      0.079717\n",
       "Harding     0.075177\n",
       "Bush        0.073158\n",
       "Wilson      0.064581\n",
       "Reagan      0.064077\n",
       "Eisenhower  0.062059\n",
       "Truman      0.058527\n",
       "Obama       0.056509\n",
       "Nixon       0.053481\n",
       "GWBush      0.048940\n",
       "Johnson     0.047427\n",
       "Roosevelt   0.042886\n",
       "Clinton     0.040868\n",
       "Carter      0.026741\n",
       "Kennedy     0.026236"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count table of sentence percentages by president, from highest to lowest. \n",
    "df_final = df.groupby('president').count()/df['sent'].count()\n",
    "df_final = df_final.sort_values(by=['sent'], ascending=False)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1486, 2)\n",
      "(496, 2)\n"
     ]
    }
   ],
   "source": [
    "#train-test split, test size kept at 25 percent\n",
    "df_train, df_test = train_test_split(df,\n",
    "                                    test_size=0.25,\n",
    "                                    random_state=40)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1486, 1200)\n",
      "(496, 1200)\n"
     ]
    }
   ],
   "source": [
    "#tf-idf vectorizer; X and Y defined\n",
    "#tf-idf value, which we hope to find in each (significant) word, includes the following: \n",
    "#document frequency, inverse document frequency(idf), term frequency(tf), and tf-idf, the product of tf and idf.\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             lowercase=True,       \n",
    "                             min_df=2,             \n",
    "                             max_df=0.5,           \n",
    "                             use_idf=True,\n",
    "                             smooth_idf=True,\n",
    "                             norm='l2',\n",
    "                             max_features=1200\n",
    "                             )\n",
    "#we kept features at 900 max.\n",
    "#we've included every word repeated more than twice per document, and discarded words that appear in 75% of documents\n",
    "X_train = df_train['sent']\n",
    "X_test = df_test['sent']\n",
    "Y_train = df_train['president']\n",
    "Y_test = df_test['president']\n",
    "\n",
    "vectorizer.fit_transform(df['sent'])\n",
    "X_train_tfidf = vectorizer.transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "             word  avg_weight\n",
      "1190       world    0.025320\n",
      "712       people    0.021569\n",
      "456           ha    0.020923\n",
      "661       nation    0.020221\n",
      "440   government    0.017588\n",
      "676          new    0.016393\n",
      "47       america    0.016049\n",
      "710        peace    0.014085\n",
      "418      freedom    0.013564\n",
      "1083        time    0.013150\n",
      "442        great    0.012977\n",
      "220      country    0.012584\n",
      "1186        work    0.012424\n",
      "576         life    0.011983\n",
      "549         know    0.010912\n",
      "\n",
      "Test:\n",
      "             word  avg_weight\n",
      "661       nation    0.026600\n",
      "456           ha    0.024627\n",
      "712       people    0.020957\n",
      "576         life    0.018546\n",
      "440   government    0.016624\n",
      "47       america    0.015001\n",
      "1190       world    0.014699\n",
      "1083        time    0.014211\n",
      "220      country    0.013876\n",
      "572          let    0.013603\n",
      "442        great    0.012953\n",
      "145       change    0.012558\n",
      "959        shall    0.012193\n",
      "611          man    0.011687\n",
      "676          new    0.011192\n"
     ]
    }
   ],
   "source": [
    "#individual words are weighted; top 15 words listed\n",
    "weights = np.asarray(X_train_tfidf.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'word': vectorizer.get_feature_names(), 'avg_weight': weights})\n",
    "print(\"Train:\\n\", weights_df.sort_values(by='avg_weight', ascending=False).head(15))\n",
    "\n",
    "weights = np.asarray(X_test_tfidf.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'word': vectorizer.get_feature_names(), 'avg_weight': weights})\n",
    "print(\"\\nTest:\\n\", weights_df.sort_values(by='avg_weight', ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the tf-idf score suggests that a word was frequently used in a small portion of sentences. Ignoring \"ha\", a data cleaning misstep, words like \"nation\", \"government\", \"peace\" and \"freedom\" indicate presidents have preferred to use inaugural addresses as means to make promises, promises to do right by American core values as well as their civic duties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured: 61.191536907593516\n",
      "Components 0:\n",
      "sent\n",
      "I am sure our own people will not misunderstand nor will the world misconstrue                                0.522835\n",
      "We have come to a new realization of our place in the world and a new appraisal of our Nation by the world    0.472248\n",
      "I also know the people of the world                                                                           0.462371\n",
      "Is our world gone                                                                                             0.433433\n",
      "Across the world we see them embraced and we rejoice                                                          0.433433\n",
      "Name: 0, dtype: float64\n",
      "Components 1:\n",
      "sent\n",
      "Across the world we see them embraced and we rejoice                                                                        0.628239\n",
      "Is our world gone                                                                                                           0.628239\n",
      "Is a new world coming                                                                                                       0.610381\n",
      "We are being forged into a new unity amidst the fire that now blaze throughout the world                                    0.574500\n",
      "Fortunately the New World is largely free from the inheritance of fear and distrust which have so troubled the Old World    0.532917\n",
      "Name: 1, dtype: float64\n",
      "Components 2:\n",
      "sent\n",
      "America                                        0.857854\n",
      "For this is what America is all about          0.857854\n",
      "The call is for productive America to go on    0.751748\n",
      "I know America                                 0.733999\n",
      "I know America s youth                         0.733999\n",
      "Name: 2, dtype: float64\n",
      "Components 3:\n",
      "sent\n",
      "We are a composite and cosmopolitan people                                                                                                                                                       0.692499\n",
      "Our people must give and take                                                                                                                                                                    0.692499\n",
      "No country is more loved by it people                                                                                                                                                            0.531428\n",
      "Powerful people maneuver for position and worry endlessly about who is in and who is out who is up and who is down forgetting those people whose toil and sweat sends u here and pave our way    0.480540\n",
      "For the impoverishment of any single people in the world mean danger to the well being of all other people                                                                                       0.424851\n",
      "Name: 3, dtype: float64\n",
      "Components 4:\n",
      "sent\n",
      "It ha now been completed                                                                                                              0.528250\n",
      "It ha been impossible to avoid them                                                                                                   0.436989\n",
      "What America ha done ha given renewed hope and courage to all who have faith in government by the people                              0.426324\n",
      "That is why our constitutional system ha proved itself the most superbly enduring political mechanism the modern world ha produced    0.425789\n",
      "Our Government ha no power except that granted it by the people                                                                       0.409339\n",
      "Name: 4, dtype: float64\n",
      "Components 5:\n",
      "sent\n",
      "The whole world is at peace                                                                 0.511134\n",
      "We are at peace with all of them                                                            0.503098\n",
      "We not only desire peace with the world but to see peace maintained throughout the world    0.488267\n",
      "We are caught in war wanting peace                                                          0.407662\n",
      "It must be a worldwide effort for the achievement of peace plenty and freedom               0.399113\n",
      "Name: 5, dtype: float64\n",
      "Components 6:\n",
      "sent\n",
      "This work continues                      0.623159\n",
      "We know what work Freedom work           0.609232\n",
      "Our work is a work of restoration        0.603141\n",
      "No altered system will work a miracle    0.535175\n",
      "My friend we have work to do             0.496399\n",
      "Name: 6, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#latent semantic analysis (LSA)\n",
    "# using single value decomposition (SVD), a dimensionality reduction tool, we truncate our max feature of 1000 by 25%. \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "svd = TruncatedSVD(250)\n",
    "lsa_pipe = make_pipeline(svd, Normalizer())\n",
    "\n",
    "X_train_lsa = lsa_pipe.fit_transform(X_train_tfidf)\n",
    "X_test_lsa = lsa_pipe.transform(X_test_tfidf)\n",
    "\n",
    "variance_explained = svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print('Percent variance captured:', total_variance*100)\n",
    "\n",
    "sent_by_component = pd.DataFrame(X_train_lsa, index=X_train)\n",
    "\n",
    "for i in range(7):\n",
    "    print('Components {}:'.format(i))\n",
    "    print(sent_by_component.loc[:, i].sort_values(ascending=False)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 25% of features, the model still captures 61% of the text's variation. While our clusters' messages are generally one of \"hope\" - for a brighter, united future, components 3 and 4 bring attention to the American people. Component 3 mentions their hard work (\"toil and sweat\") and cosmopolitan nature, whereas Component 4 also draws attention to their innocence and good intentions (\"our own people will not misunderstand nor will the world misconstrue\", \"Our people must give and take\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning: RFC, LR, SVC and Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will now move on to Supervised Learning, using our generated features to obtain Classification accuracy scores. \n",
    "def Classify(clf):\n",
    "    X_train = X_train_tfidf\n",
    "    X_test = X_test_tfidf\n",
    "    clf.fit(X_train, Y_train)\n",
    "    print('TF-IDF:')\n",
    "    print('Train accuracy:', clf.score(X_train, Y_train))\n",
    "    print('Test accuracy:', clf.score(X_test, Y_test))\n",
    "    print('Cross Validation:', cross_val_score(clf, X_train, Y_train, cv=5))\n",
    "    \n",
    "    X_train = X_train_lsa\n",
    "    X_test = X_test_lsa\n",
    "    clf.fit(X_train, Y_train)\n",
    "    print('LSA:')\n",
    "    print('Train accuracy:', clf.score(X_train, Y_train))\n",
    "    print('Test accuracy:', clf.score(X_test, Y_test))\n",
    "    print('Cross Validation:', cross_val_score(clf, X_train, Y_train, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF:\n",
      "Train accuracy: 0.2126514131897712\n",
      "Test accuracy: 0.1350806451612903\n",
      "Cross Validation: [0.14144737 0.1589404  0.17449664 0.14383562 0.15862069]\n",
      "LSA:\n",
      "Train accuracy: 0.3882907133243607\n",
      "Test accuracy: 0.13306451612903225\n",
      "Cross Validation: [0.13815789 0.16225166 0.16778523 0.15068493 0.16896552]\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100,\n",
    "                             max_depth=4,\n",
    "                             random_state=42,\n",
    "                             class_weight=None \n",
    "                            )   \n",
    "Classify(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF:\n",
      "Train accuracy: 0.8600269179004038\n",
      "Test accuracy: 0.27419354838709675\n",
      "Cross Validation: [0.36513158 0.34437086 0.32214765 0.29452055 0.37586207]\n",
      "LSA:\n",
      "Train accuracy: 0.6049798115746972\n",
      "Test accuracy: 0.2762096774193548\n",
      "Cross Validation: [0.33223684 0.31125828 0.30536913 0.28082192 0.35517241]\n"
     ]
    }
   ],
   "source": [
    "#Logisitc Regression\n",
    "clf_1 = LogisticRegression(penalty='l2',\n",
    "                           fit_intercept=False,\n",
    "                           class_weight=None,\n",
    "                           random_state=42, \n",
    "                           solver='lbfgs'\n",
    "                          )\n",
    "Classify(clf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF:\n",
      "Train accuracy: 0.09757738896366083\n",
      "Test accuracy: 0.10483870967741936\n",
      "Cross Validation: [0.09539474 0.09602649 0.09731544 0.09931507 0.1       ]\n",
      "LSA:\n",
      "Train accuracy: 0.09757738896366083\n",
      "Test accuracy: 0.10483870967741936\n",
      "Cross Validation: [0.09539474 0.09602649 0.09731544 0.09931507 0.1       ]\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Classifier\n",
    "clf_2 = SVC(C=0.5, \n",
    "            class_weight=None\n",
    "           )\n",
    "    \n",
    "Classify(clf_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF:\n",
      "Train accuracy: 0.509421265141319\n",
      "Test accuracy: 0.18951612903225806\n",
      "Cross Validation: [0.18092105 0.20529801 0.22483221 0.21917808 0.2137931 ]\n",
      "LSA:\n",
      "Train accuracy: 0.6345895020188426\n",
      "Test accuracy: 0.1532258064516129\n",
      "Cross Validation: [0.15460526 0.13576159 0.16107383 0.14726027 0.15517241]\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "clf_3 = GradientBoostingClassifier(learning_rate=0.01)\n",
    "Classify(clf_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and testing accuracy scores leave much to be desired. Despite tweaking the parameters for each, there are clear signs of overfitting - particularly for Logistic regression and Gradient Boosting. Although SVC's scores were the lowest, it showed a bit of underfitting. The accuracy scores were generally low, and even with adjustments, none of the supervised learning technique cracked the expected/desired score of 0.45. Logistic Regression had the highest accuracy, with LSA producing lesser overfitting. However, Cross-validation scores unsurprisingly indicated considerable overfitting only for Logistic Regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project was particularly challenging for two main reasons. First, I wasn't sure if the data cleaning method was best suited for this set of texts. Second, the reason for a less-than-desirable accuracy score was not immediately clear to me. A large part of what could be improved are the following: \n",
    "- better addressing the class imbalances among inaugural address being used (as noted earlier)\n",
    "- better identifying and removing stop words that have an adverse affect on accuracy scoring\n",
    "- choosing a larger dataset or incorporating more text files from inaugural.fileids(); prior to exploring the 17 data texts, I did not anticipate the individual text files to be so small, but for the sake of computer memory, decided to commit to the initial dataset size. \n",
    "\n",
    "Some interesting observations - compared to supervised learning, our two unsupervised learning techniques generated far more insight into the dataset's semantics. They definitely stood out in comparison to Random Forest Classifier, its greatest drawback being its \"black box\", hard-to-make-sense-of nature. While adjusting the parameters for supervised learning has done little to improve the overall scores, they have increased the scores/addressed overfitting issues for some. Lowering the learning_rate for gradient boosting, for example, reduced the train accuracy from the high 90's to 50 and 63, lessening a glaring overfitting problem. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
